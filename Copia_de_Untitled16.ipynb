{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+S1m0Q18yTp9cyVpjHpOz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsebastianber12/hola123/blob/main/Copia_de_Untitled16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlWXau2dTJ0X"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive y cargar datos\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Librerías necesarias\n",
        "import numpy as np\n",
        "import h5py\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Flatten, Concatenate, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Cargar datos desde Google Drive\n",
        "data_path = \"/content/gdrive/MyDrive/Project_2024-2\"\n",
        "\n",
        "# Cargar datos de entrenamiento\n",
        "dataset = h5py.File(data_path + '/training_set_500.h5', 'r')\n",
        "X_train = np.array(dataset.get('data'))\n",
        "y_train = np.array(dataset.get('labels'))\n",
        "dataset.close()\n",
        "\n",
        "# Normalización de datos\n",
        "X_train = X_train / np.max(X_train)\n",
        "\n",
        "# Convertir etiquetas a formato one-hot\n",
        "nclases = 3\n",
        "y_train = to_categorical(y_train, nclases)\n",
        "\n",
        "# Cargar datos de prueba\n",
        "dataset = h5py.File(data_path + '/test_without_labels_200.h5', 'r')\n",
        "X_test = np.array(dataset.get('data'))\n",
        "dataset.close()\n",
        "\n",
        "# Normalización de datos de prueba\n",
        "X_test = X_test / np.max(X_test)\n",
        "\n",
        "# Función para crear ventanas deslizantes\n",
        "def create_sliding_windows(data, window_size, stride):\n",
        "    windows = []\n",
        "    for signal in data:\n",
        "        signal_windows = [\n",
        "            signal[i:i+window_size]\n",
        "            for i in range(0, len(signal) - window_size + 1, stride)\n",
        "        ]\n",
        "        windows.append(signal_windows)\n",
        "    return np.array(windows)\n",
        "\n",
        "# Parámetros para ventanas deslizantes\n",
        "window_size = 256\n",
        "stride = 128\n",
        "\n",
        "# Crear ventanas deslizantes para entrenamiento y prueba\n",
        "X_train_windows = create_sliding_windows(X_train.squeeze(-1), window_size, stride).reshape(-1, window_size, 1)\n",
        "X_test_windows = create_sliding_windows(X_test.squeeze(-1), window_size, stride).reshape(-1, window_size, 1)\n",
        "\n",
        "# Ajustar etiquetas para ventanas\n",
        "y_train_windows = np.repeat(y_train, X_train_windows.shape[0] // X_train.shape[0], axis=0)\n",
        "\n",
        "# Función para calcular intervalos RR\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "def calculate_rr_intervals(signals, fs=250):\n",
        "    rr_intervals = []\n",
        "    for signal in signals:\n",
        "        peaks, _ = find_peaks(signal.flatten(), height=0.5, distance=fs*0.6)\n",
        "        rr = np.diff(peaks) / fs\n",
        "        rr_intervals.append(rr[:10] if len(rr) >= 10 else np.pad(rr, (0, 10 - len(rr)), constant_values=0))\n",
        "    return np.array(rr_intervals)\n",
        "\n",
        "X_train_rr = calculate_rr_intervals(X_train.squeeze(-1))\n",
        "X_test_rr = calculate_rr_intervals(X_test.squeeze(-1))\n",
        "\n",
        "# Normalizar intervalos RR\n",
        "X_train_rr = X_train_rr / np.max(X_train_rr)\n",
        "X_test_rr = X_test_rr / np.max(X_test_rr)\n",
        "\n",
        "# Repetir intervalos RR para que coincidan con las ventanas deslizantes\n",
        "X_train_rr_repeated = np.repeat(X_train_rr, X_train_windows.shape[0] // X_train_rr.shape[0], axis=0)\n",
        "X_test_rr_repeated = np.repeat(X_test_rr, X_test_windows.shape[0] // X_test_rr.shape[0], axis=0)\n",
        "\n",
        "# Dividir datos en entrenamiento y validación\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_windows, y_train_windows, test_size=0.2, random_state=42\n",
        ")\n",
        "X_train_rr_split, X_val_rr_split = train_test_split(X_train_rr_repeated, test_size=0.2, random_state=42)\n",
        "\n",
        "# Función para crear el modelo CNN + LSTM optimizado\n",
        "def create_cnn_lstm_model_optimized(window_size, rr_size):\n",
        "    ecg_input = Input(shape=(window_size, 1), name='ECG_Input')\n",
        "\n",
        "    # Bloque CNN ajustado\n",
        "    x = Conv1D(16, kernel_size=3, kernel_initializer='he_normal')(ecg_input)\n",
        "    x = LeakyReLU(alpha=0.1)(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Conv1D(32, kernel_size=3, kernel_initializer='he_normal')(x)\n",
        "    x = LeakyReLU(alpha=0.1)(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    rr_input = Input(shape=(rr_size,), name='RR_Input')\n",
        "    y = Dense(32, kernel_initializer='he_normal')(rr_input)\n",
        "    y = LeakyReLU(alpha=0.1)(y)\n",
        "    y = Dropout(0.3)(y)\n",
        "\n",
        "    combined = Concatenate()([x, y])\n",
        "\n",
        "    z = Dense(64, kernel_initializer='he_normal')(combined)\n",
        "    z = LeakyReLU(alpha=0.1)(z)\n",
        "    z = Dropout(0.4)(z)\n",
        "\n",
        "    z = Dense(32, kernel_initializer='he_normal')(z)\n",
        "    z = LeakyReLU(alpha=0.1)(z)\n",
        "    z = Dropout(0.4)(z)\n",
        "\n",
        "    output = Dense(nclases, activation='softmax', name='Output')(z)\n",
        "\n",
        "    model = Model(inputs=[ecg_input, rr_input], outputs=output)\n",
        "    return model\n",
        "\n",
        "# Crear modelo optimizado\n",
        "model = create_cnn_lstm_model_optimized(window_size=256, rr_size=10)\n",
        "\n",
        "# Verificar número de parámetros entrenables\n",
        "model.summary()\n",
        "\n",
        "# Compilar modelo con Nadam\n",
        "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    [X_train_split, X_train_rr_split],\n",
        "    y_train_split,\n",
        "    validation_data=([X_val_split, X_val_rr_split], y_val_split),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Realizar predicciones finales\n",
        "y_pred = model.predict([X_test_windows, X_test_rr_repeated])\n",
        "\n",
        "# Promediar predicciones para reducir al tamaño original\n",
        "n_windows_per_signal = X_test_windows.shape[0] // X_test.shape[0]\n",
        "y_pred_reshaped = y_pred.reshape(X_test.shape[0], n_windows_per_signal, -1)\n",
        "y_pred_avg = np.mean(y_pred_reshaped, axis=1)\n",
        "\n",
        "# Convertir a formato One-Hot Encoding\n",
        "y_pred_classes = np.argmax(y_pred_avg, axis=1)\n",
        "y_pred_one_hot = to_categorical(y_pred_classes, num_classes=nclases)\n",
        "\n",
        "# Calcular F1-Score en datos de validación\n",
        "y_val_pred = model.predict([X_val_split, X_val_rr_split])\n",
        "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
        "y_val_true_classes = np.argmax(y_val_split, axis=1)\n",
        "\n",
        "f1 = f1_score(y_val_true_classes, y_val_pred_classes, average='weighted')\n",
        "print(f\"F1-Score en validación: {f1:.4f}\")\n",
        "\n",
        "# Guardar modelo y predicciones\n",
        "model.save('/content/gdrive/MyDrive/GROUP_C1_2171513_2172323_Carlos_Bautista_Sebastian_Bernal_model.h5')\n",
        "np.save('/content/gdrive/MyDrive/GROUP_C1_2171513_2172323_Carlos_Bautista_Sebastian_Bernal_prediction_fixed', y_pred_one_hot)\n",
        "\n",
        "print(\"\\nModelo y predicciones guardados exitosamente.\")"
      ]
    }
  ]
}